\documentclass[12pt]{article}
\usepackage[letterpaper]{geometry}


\input{macros}

\usepackage{proof-dashed}
\usepackage{tikz-cd}
\usepackage{booktabs}


\begin{document}

\title{9/18/2013 Lecture Notes}
\author{Enoch Cheung}
\date{}

\maketitle

\section{Proof Terms}

We wish to study propositions along with their proof as mathematical objects. In the type theoretic framework, we can use the notation $M:A$ where $A$ is a proposition and $M$ is a proof of $A$. We will see that this corresponds to the category theoretic notion of a mapping $M:A\to B$. Another important notion is  the identity of proofs, which will be denoted $M\equiv N:A$ where $M,N$ are equivalent proofs of $A$. This will correspond in the category theoretic contex to two maps form $A$ to $B$ being equal $M=N:A\to B$.

\subsection{Proof Terms as Variables}

We can combine the idea of keeping track of proofs with our previous notion of entailment. If $A_1,\dots,A_n$ entails $A$, meaning that $A_1,\dots, A_n\vdash A$, there will be a proof $M$ of $A$ that uses the propositions $A_1,\dots,A_n$. Thus, we will write
\[
x_1:A_1,\dots, x_n:A_n \vdash M:A
\] 
where each $x_i:A_i$ is a proof term. We can think of the proof terms $x_1,\dots,x_n$ as hypothesise for the proof, but what we really want is for them to behave as variables. $M$ then uses the variables $x_1,\dots,x_n$ to prove $A$, so $M$ would encapsulate the grammar a proof that uses variables $x_1,\dots,x_n$.




Instead of proving a proposition $A$ from nothing, most of the time $A$ will rely on other propositions $A_1,\dots, A_n$.

\subsection{Structural Properties of Entailment with Proof Terms}

Now that we have proof terms, we can see how they act as variables by examining their interaction with the structural properties of entailment. We will also keep track of other assumptions/context $\Gamma,\Gamma'$ to demonstrate that the structural properties will hold in the presence of assumptions.

\paragraph{Reflexivity / Variables Rule}
Reflexivity tells us that $A$ should entail $A$, so now that we have a variable $x:A$ that proves $A$, the variable should be carried through. We can think of this as the variables rule.
\begin{equation*}
  \infer[\text{\textsf{R/V}}]{\Gamma, x:A,\Gamma' \entails x:A}{
    } \,
\end{equation*}


\paragraph{Transitivity / Substitution}
Transitivity tells us that if $A$ is true and $B$ follows from $A$, then $B$ is true. In terms of proofs, if we have a proof $N:A$ and a proof $N:B$ which uses a variable $x$ that is supposed to prove $A$, then we can substitute the proof $M:A$ into $N:B$ to prove $B$. Since we are substituting $M$ into $x$ inside $N$, we denote this substitution $[M/x]N:B$.
\begin{equation*}
  \infer[\text{\textsf{T/S}}]{\Gamma,\Gamma'\entails [M/x]N:B}{
    \Gamma,x:A,\Gamma' \entails N:B &
    \Gamma \entails A}
\end{equation*}

\paragraph{Weakening}
\begin{equation*}
  \infer[\text{\textsf{W}}]{\Gamma,\Gamma' \entails M:A}{
    \Gamma \entails M:A}
\end{equation*}

\paragraph{Contraction}
If $N:B$ follows from $A$ using two different proofs $x:A, y:A$ for $A$, can just pick one $z=x$ or $z=y$ as the proof of $z:A$ and use it in the instances of variables $x,y$ in $N:B$
\begin{equation*}
  \infer[\text{\textsf{C}}]{\Gamma,z:A,\Gamma' \entails [z,z/x,y]N:B}{
  \Gamma,x:A,y:A,\Gamma'\entails N:B}
\end{equation*}

\paragraph{Exchange}
\begin{equation*}
  \infer[\text{\textsf{X}}]{\Gamma,y:B,x:A,\Gamma' \entails N:C}{
  \Gamma,x:A,y:B,\Gamma'\entails N:C}
\end{equation*}

\subsection{Negative Fragment of IPL with Proof Terms}

We want to look at what happens to the Negative Fragment of IPL when we consider proof terms. Here are the important ones:

\paragraph{Truth Introduction} Truth is trivially true, so we have
\begin{equation*}
  \infer[\top I]{\Gamma\entails \langle\ \rangle:\top}{
  }
\end{equation*}

\paragraph{Conjunction Introduction} We combine the proofs $M:A$ and $N:B$ into $\langle M,N\rangle : A\wedge B$
\begin{equation*}
  \infer[\wedge I]{\Gamma\entails \langle M,N \rangle:A\wedge B}{
  \Gamma\entails M:A
  & \Gamma\entails N:B}
\end{equation*}

\paragraph{Conjunction Elimination} We can recover from a proof $M:A\wedge B$ proofs of $A$ and $B$
\begin{mathpar}
  \infer[\wedge E_1]{\Gamma\entails \operatorname{fst}(M):A}{
	\Gamma\entails M:A\wedge B}
\and
	\infer[\wedge E_2]{\Gamma\entails \operatorname{snd}(M):B}{\Gamma\entails M:A\wedge B}
\end{mathpar}

\paragraph{Implication Introduction} If we have a proof $M:B$ that uses $x:A$ as a variable, then we can consider $\lambda x.M$ as a function that maps $x$ a variable to a proof of B that uses $x$, which proves that $B\supset A$
\begin{equation*}
  \infer[{\supset} I]{\Gamma \entails \lambda x.M:A\supset B}{\Gamma,x:A \entails M:B}
\end{equation*}

\paragraph{Implication Elimination} By applying an actual proof $N:A$ to the function described above, we obtain a proof $M(N):B$
\begin{equation*}
  \infer[{\supset} E]{\Gamma \entails M(N):B}{\Gamma \entails M:A\supset B & \Gamma\entails N:A}
\end{equation*}

\section{Identity of Proofs}

\subsection{Definitional Equality}

We want to think about when two proofs $M:A$ and $M':A$ are the same. We will introduce an equivalence relation called \emph{definitional equality} that respects the proof rules, denoted $M\equiv M':A$. We want definitional equality $\equiv$ to be the least congruence containing (closed under) the $\beta$ rules. We will define what this means:

A \emph{congruence} is an equivalence relation that respects our operators. Being an equivalence relation that it is reflexive ($M\equiv M:A$), symmetric ($M\equiv N:A$ implies that $N\equiv M:A$), and transitive ($M\equiv N:A$ and $N\equiv M':A$ implies that $M\equiv M':A$).

For the equivalence relation to respect our operators basically means that if $M\equiv M':A$, then that their image under any operator should be equivalent. In other words, we should be able to replace $M$ with $M'$ everywhere. For example
\[
\infer[]{\Gamma\entails \operatorname{fst}(M)\equiv\operatorname{fst}(M'):A}{\Gamma \entails M\equiv M':A\wedge B}
\]

There can be many congruences that contains the $\beta$ rules. Given two congruences $\equiv$ and $\equiv'$, we say $\equiv$ is finer than $\equiv'$ if ${{M\equiv' N}:A}$ implies that $M\equiv N:A$. The least congruence that contains the proof rules is the finest congruence that contains the $\beta$ rules. We will define the $\beta$ rules in the next section.

We will give a more explicit definition to definitional equality later.

\subsection{Gentzen's Inversion Principle}

Gentzen's Inversion Principle captures the idea that ``elim is post-inverse to intro," which is the informal notion that the elimination rules should cancel the introduction rules, modulo definitional equality. The following are the $\beta$ rules for the negative fragment of IPL:

\paragraph{Conjunction}
When we introduce a conjunction, we combine proofs $M:A$ and $N:B$ to produce a proof $\langle M,N\rangle :A\wedge B$. When we eliminate a conjunction, we retrieve $M:A$ or $N:B$. We do not want this process to alter our original $M$ or $N$
\[
\infer[\beta\wedge_1]{\Gamma\vdash \operatorname{fst}(\langle M,N\rangle)\equiv M:A}{\Gamma\vdash M:A & \Gamma\vdash N:B}
\]

\[
\infer[\beta\wedge_2]{\Gamma\vdash \operatorname{snd}(\langle M,N\rangle)\equiv N:A}{\Gamma\vdash M:A & \Gamma\vdash N:B}
\]

\paragraph{Implication}
When we introduce an implication, we convert a proof $M:B$ which uses some variable $x:A$ to a function which uses a variable $x$ to produce a proof of $B$. When we eliminate implication, we apply the proof of $A\supset B$ to $N:A$ to produce a proof of $B$.
\[
\infer[\beta{\supset}]{\Gamma\vdash(\lambda x.M)(N)\equiv [N/x]M:B}{\Gamma,x:A\vdash M:B & \Gamma\vdash N:A}
\]

\subsection{Gentzen's Unicity Principle}

Gentzen's Unicity Princples on the other hand captures the idea that ``intro is post-inverse to elim.'' Another way to think about it is that there should be only one way modulo definitional equivalence to prove something, which is the way we have described. They are the $\eta$ rules, which are the following

\paragraph{Truth}
\[
\infer[\eta\top]{\Gamma\vdash M\equiv \langle \ \rangle: \top}{\Gamma\vdash M:\top}
\]

\paragraph{Conjunction}
\[
\infer[\eta\wedge]{\Gamma\vdash M\equiv \langle \operatorname{fst}(M),\operatorname{snd}(N)\rangle :A\wedge B }{\Gamma\vdash M:A\wedge B}
\]

\paragraph{Implication}
\[
\infer[\eta{\supset}]{\Gamma\vdash M\equiv \lambda x.Mx: A\supset B}{M:A\supset B}
\]

\section{Proposition as Types}



There is a correspondence between propositions and types:
\begin{center}
  \begin{tabular}{@{} cc @{}}
    \toprule
    Propositions & Types \\ 
    \midrule
    $\top$ & $1$ \\ 
    $A\wedge B$ & $A\times B$ \\ 
    $A\supset B$ & function $A\to B$ or $B^A$ \\ 
    $\bot$ & $0$ \\ 
    $A\vee B$ & $A+B$\\
    \bottomrule
  \end{tabular}
\end{center}

For now, note that meets like $\top$ and $A\wedge B$ corresponds to products like $1$ and $A\times B$, and joins like $\bot$ and $A\vee B$ corresponds to coproducts like $0$ and $A+B$. This correspondence should become more apparent as we go along. We will now introduce the objects on the right column.


\section{Category Theoretic Approach}

In a Heyting Algebra, we have a preorder $A\leq B$ when $A$ implies $B$. However, we now wish to keep track of proofs, so if $M$ is a proof from $A$ to $B$, we want to think of it as a map $M:A\to B$.

\paragraph{Identity} There should be an identity map
\[
\operatorname{id}:A\to A
\]

\paragraph{Composition} We should be able to compose maps
\[
\infer[]{f\circ g:A\to B}{g:B\to C & f:A\to B}
\]

\paragraph{Coherence Conditions} The identity map and composition of maps should behave like functions
\begin{align*}
\operatorname{id}_B\circ f &=f:A\to B	\\
f\circ \operatorname{id}_A &=f:A\to B	\\
f\circ(g\circ h) &= (f\circ g)\circ h: A\to D
\end{align*}

Now we can think about objects in the category that corresponds to propostions given in the correspondence.

\paragraph{Terminal Object}
$1$ is the terminal object, also called the final object, which corresponds to $\top$. For any object $A$ there is a unique map $A\to 1$. This corresponds to $\top$ being the the greatest object in a Heyting Algebra, meaning that for all $A$, $A\leq 1$).

Existence:
\[
\langle \ \rangle:A\to 1
\]

Uniqueness:
\[
\infer[\eta\top]{M:A\to 1}{M=\langle \ \rangle:A\to 1}
\]

\paragraph{Product} For any objects $A$ and $B$ there is an object $C=A\times B$ that is the \emph{product} of $A$ and $B$, which corresponds to the join $A\wedge B$. The product $A\times B$ has the following universal property:
\begin{equation*}
  \begin{tikzcd}
    {} & D \arrow[bend right]{ddl}[swap]{M} \arrow[bend left]{ddr}{N}\dar[dashed]{\langle M,N\rangle} & {} \\
    {} & A \times B \dlar{\text{fst}}\drar[swap]{\text{snd}} & {} \\
    A & {} & B
  \end{tikzcd}
\end{equation*}
where the diagram commutes.

First, the existence condition means that there are maps
\begin{align*}
\operatorname{fst}&:A\times B\to A	\\
\operatorname{snd}&:A\times B\to B
\end{align*}

The universal property says that for every object $D$ such that $M:D\to A$ and $N:D\to B$, there exists a unique map $\langle M,N\rangle: D\to A\times B$ such that
\[
\infer[]{\langle M,N\rangle: D\to A\times B}{M:D\to A & N:D\to B}
\]
and the diagram communtes meaning
\begin{align*}
\operatorname{fst}\circ\langle M,N\rangle = M:D\to A 	\qquad (\beta {\times_1})		\\
\operatorname{snd}\circ\langle M,N\rangle = N:D\to B 	\qquad (\beta {\times_2})	
\end{align*}

Furthermore, the map $\langle M,N\rangle:D\to A\times B$ is unique in the sense that
\[
\infer[\eta\times]{P=\langle M,N\rangle : D\to A\times B}{P:D\to A\times B & \operatorname{fst}\circ P=M:D\to A & \operatorname{snd}\circ P=N:D\to B}
\]
so in other words $\langle\operatorname{fst}\circ P,\operatorname{snd}\circ P\rangle =P$.

Another way to say the above is
\begin{align*}
\langle \operatorname{fst},\operatorname{snd}\rangle &= \operatorname{id}	\\
\langle M,N\rangle\circ P &= \langle M\circ P,N\circ P\rangle
\end{align*}

\paragraph{Exponentials}
Given objects $A$ and $B$, an exponential $B^A$ (which corresponds to $A\supset B$) is an object with the following universal property:
\[
\begin{tikzcd}
C \arrow[dashed]{dd}[swap]{\lambda(h)}&{C\times A}\arrow{ddrr}{h} \arrow[dashed]{dd}[swap]{\lambda(h)\times \operatorname{id}_A} &&\\
&& {}	&{}\\
B^A&{B^A\times A}\arrow{rr}[swap]{\operatorname{ap}} &{}&{B}
\end{tikzcd}
\]
such that the diagram commutes.

This means that there exists a map $\operatorname{ap}:B^A\times A\to B$ (application map) that corresponds to implication elimination.

The universal property is that for all object $C$ that has a map $h:C\times A\to B$, there exists a unique map $\lambda(h):C\to B^A$ such that
\[
(\lambda(h)\times \operatorname{id}_A )\circ \operatorname{ap}=h:C\times A\to B
\]
This means that the diagram commutes. Another way to express the induced map is $\lambda(h)\times \operatorname{id}_A = \langle \lambda(h)\circ\operatorname{fst},\operatorname{snd}\rangle$.

The map $\lambda(h):C\to B^A$ is unique, meaning that
\[
\infer[\eta]{g=\lambda(h):C\to B^A}{\operatorname{ap}\circ(g\times \operatorname{id_A})=h:C\times A\to B}
\]
\end{document}
